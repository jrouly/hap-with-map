/**
 * Copyright 2013 AMALTHEA REU; Dillon Rose; Michel Rouly
 * 
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package root.output;

import java.io.IOException;
import java.net.InetAddress;
import java.net.URI;
import java.net.URISyntaxException;
import java.net.UnknownHostException;
import java.sql.BatchUpdateException;
import java.sql.CallableStatement;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hive.jdbc.HiveDriver;
import org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.util.ToolRunner;
import org.apache.mahout.common.AbstractJob;
import org.apache.mahout.math.VectorWritable;


/**
 * <p>
 * This is the driver class for the Data Output directive.
 * </p>
 * 
 * <p>
 * This job serves to take the data generated by the Hierarchical Affinity 
 * Propagation algorithm and consolidate it into a Hive and mySQL database.
 * </p>
 * 
 * @author Dillon Rose
 * @author Michel Rouly
 * 
 * @since 2013.06.25
 * 
 */
public class OutputJob extends AbstractJob {

	// -------------------------------------------------------------------
	// The following configuration variables will have default values.
	private static String dirHiveDateRoot = "/hive";
	private static String dirAPOut = "affinityPropagation";
	private static String dirWordDict = "word-dictionary";
	private static String dirMetadata = "file-dictionary";
	private static String dirTFVectors = "vectorFiles/tf-vectors";
	private static String topNWords = "100";

	// -------------------------------------------------------------------
	// The following configuration variables must be set by the user.
	private static String url_mysql; // mysql://localhost:3306/db_presentation
	private static String usr_mysql; // amalthea
	private static String psw_mysql; // Bwn8wZxBHS2Tdxe3

	private static String url_hive; // hive://localhost:10000/default

	// -------------------------------------------------------------------
	// The following are environment variables, not to be configured.
	private FileSystem fs;
	private URI uri;
	private Configuration conf;

	private static String hiveDriver = HiveDriver.class.getName();
	private static String inputFormat = KeyAsValueSequenceFileInputFormat.class
			.getName();
	private static String outputFormat = HiveIgnoreKeyTextOutputFormat.class
			.getName();
	private static String dir_joinWordFreq = "wordFrequencies";

	// Map of DocID -> UUIDs for use in this class.
	private Map<String, String> uuids;

	// Dataset ID
	private int dataset;

	// Connection objects
	private Connection mysql, hive;


	/*
	 * Construct arguments list.
	 */
	private void addArguments() {

		addOption(
				"sqldb",
				"sqldb",
				"Full location of output SQL database [ex. mysql://domain:port/schema]",
				true);
		addOption("sqlusr", "sqlusr", "[Optional] SQL database user");
		addOption("sqlpsw", "sqlpsw", "[Optional] SQL database password");
		addOption("dataroot", "dataroot",
				"[Optional] Root data storage directory", "/hive");
		addOption("ap-out", "ap-out", "[Optional] AP-Output storage directory",
				"affinityPropagation");
		addOption("word-dict", "word-dict", "[Optional] Word dictionary file",
				"word-dictionary");
		addOption("metadata", "metadata",
				"[Optional] Document metadata storage directory", "file-dictionary");
		addOption("tf-vec", "tf-vec",
				"[Optional] Term Frequency storage directory", "vectorFiles/tf-vectors");
		addOption("nwords", "nwords",
				"[Optional] Top N most frequent words to store", "100");

	}


	/*
	 * Grab arguments from the user.
	 */
	private void initArguments() {

		url_mysql = getOption("sqldb");
		usr_mysql = getOption("sqlusr");
		psw_mysql = getOption("sqlpsw");
		dirHiveDateRoot = getOption("dataroot");
		dirAPOut = getOption("ap-out");
		dirWordDict = getOption("word-dict");
		dirMetadata = getOption("metadata");
		dirTFVectors = getOption("tf-vec");
		topNWords = getOption("nwords");

		url_mysql = "jdbc:" + url_mysql;
		url_hive = "jdbc:" + url_hive;

		dir_joinWordFreq = dirHiveDateRoot + "/" + dir_joinWordFreq;

	}


	/**
	 * This method allows the Job to act as a {@link ToolRunner} and 
	 * interface properly with the Driver.
	 * 
	 * @param args Configuration arguments
	 * @return Exit status
	 * @see ToolRunner
	 */
	@Override
	public int run(String[] args) throws Exception {

		// Try to grab the current IP Address of this machine,
		// ie. the Hive server IP.
		try { 
			String hostAddress = InetAddress.getLocalHost().getHostAddress();
			url_hive = "hive://" + hostAddress + ":10000/default";
		} catch( UnknownHostException e ) { 
			System.err.println( "[ERROR]: Unable to initialize Hive server." );
			System.err.println( e );
			return -1;
		}

		addArguments();

		if (parseArguments(args) == null) {
			return -1;
		}

		initArguments();

		System.out.println();

		printHiveJobArgs();

		initializeEnvironment();

		// Begin processing data.
		try {

			// Find the current dataset ID to use.
			dataset = getDataset() + 1;

			System.out.println("[INFO]: Using dataset: " + dataset);

			// Create and execute batch query: ins_document
			ins_document(true);
			uuids = getUUIDs();

			// Create and execute batch query: ins_word
			ins_word(true);

			// Create and execute batch query: ins_wordFreq
			ins_wordFreq(true);

			// Create and execute batch query: ins_cluster
			ins_cluster(true);

			// Create and execute batch query: ins_clusterSize
			ins_clusterSize(true);

		} catch (SQLException e) {
			System.err.println("[ERROR]: Unable to carry out batch queries.");
			System.err.println(e);
			System.err.println("[ERROR]: Terminating . . .");
			System.exit(1);
		}

		// Cleanup.
		try {
			hive.close();
			mysql.close();
		} catch (SQLException e) {
			System.err.println("[WARN]: Error shutting down database connection.");
			System.err.println( e );
			System.exit( 1 );
		}

		return 0;
	}


	/*
	 * Print the configuration values to standard output.
	 */
	private void printHiveJobArgs() {
		System.out.println();
		System.out.println("===============");
		System.out.println("Running HiveJob");
		System.out.println("===============");
		System.out.println();
		System.out.println("\t--sqldb\t\t" + url_mysql);
		System.out.println("\t--sqlusr\t" + usr_mysql);
		System.out.println("\t--sqlpsw\t" + psw_mysql);
		System.out.println("\t--dataroot\t" + dirHiveDateRoot);
		System.out.println("\t--ap-out\t" + dirAPOut);
		System.out.println("\t--word-dict\t" + dirWordDict);
		System.out.println("\t--metadata\t" + dirMetadata);
		System.out.println("\t--tf-vec\t" + dirTFVectors);
		System.out.println("\t--nwords\t" + topNWords);
		System.out.println("");
	}


	/*
	 * Attempt to establish a clean presence given the configured environment.
	 * 
	 * Specifically: 
	 * 	ensure that the DFS is clean and all specified data directories exist,
	 *  establish the ability to connect to Hive and mySQL databases,
	 *  establish connections to the Hive and mySQL databases,
	 *  remove any old Hive data tables that may have been created earlier.
	 */
	private void initializeEnvironment() {
		// Make sure the filesystem is set up and defined properly.
		try {
			// grab a copy of the File System
			conf = getConf();
			uri = new URI(conf.get("fs.default.name"));
			fs = FileSystem.get(uri, conf);
			String fsbase = fs.getWorkingDirectory() + "/";

			// Prepend the filesystem root to every directory, just in case.
			dirAPOut = fsbase + dirAPOut;
			dirMetadata = fsbase + dirMetadata;
			dirTFVectors = fsbase + dirTFVectors;
			dirWordDict = fsbase + dirWordDict;
			dir_joinWordFreq = fsbase + dir_joinWordFreq;

			// Establish that input directories exist.
			if (!fs.exists(new Path(dirHiveDateRoot))
					|| !fs.exists(new Path(dirAPOut))
					|| !fs.exists(new Path(dirMetadata))
					|| !fs.exists(new Path(dirTFVectors))
					|| !fs.exists(new Path(dirWordDict))) {
				System.err
				.println("[ERROR]: Invalid hive data directories specified."
						+ " Terminating. . .");
				System.exit(1);
			}
		} catch (IOException e) {
			System.err.println("[ERROR]: Unable to reference HDFS.");
			System.err.println(e);
			System.exit(1);
		} catch (URISyntaxException e) {
			System.err.println("[ERROR]: Unable to reference HDFS, bad URI.");
			System.err.println(e);
			System.exit(1);
		}

		// Establish that Hive driver classes exists.
		try {
			Class.forName(hiveDriver);
			Class.forName(inputFormat);
			Class.forName(outputFormat);
		} catch (ClassNotFoundException e) {
			System.err.println("[ERROR]: No driver class found on CLASSPATH.");
			e.printStackTrace();
			System.exit(1);
		}

		// Establish a connection with the mySQL database.
		try {
			System.out.print("[INFO]: Connecting to mySQL database ... ");
			mysql = DriverManager
					.getConnection(url_mysql, usr_mysql, psw_mysql);
			System.out.println("success!");
		} catch (SQLException e) {
			System.err.println("[ERROR]: Unable to connect to mySQL database.");
			System.err.println(e);
			System.exit(1);
		}

		// Establish a connection with the Hive database.
		try {
			System.out.print("[INFO]: Connecting to Hive database ... ");
			hive = DriverManager.getConnection(url_hive);
			System.out.println("success!");
		} catch (SQLException e) {
			System.err.println("[ERROR]: Unable to connect to Hive database.");
			System.err.println(e);
			System.exit(1);
		}

		// Clean up any existing Hive tables created by this script, then
		// create tables/indexes over the input directories.
		try {
			cleanHiveTables();
			createHiveTables();
		} catch (SQLException e) {
			System.err.println("[ERROR]: Unable to set up Hive environment.");
			System.err.println(e);
			System.exit(1);
		}
	}


	/*
	 * Remove any old Hive data tables that may have been created by this 
	 * script in an earlier iteration.
	 */
	private void cleanHiveTables() throws SQLException {

		Statement hive_stmt = hive.createStatement();
		String query;

		// Clean up tables: metadata,
		// exemplars,
		// word_dict,
		// tf_vec,
		System.out.println("[INFO]: Cleaning up tables:");
		{
			System.out.print("\t'metadata' ... ");
			query = "drop table metadata";
			hive_stmt.executeQuery(query);
			System.out.println("success!");

			System.out.print("\t'exemplars' ... ");
			query = "drop table exemplars";
			hive_stmt.executeQuery(query);
			System.out.println("success!");

			System.out.print("\t'word_dict' ... ");
			query = "drop table word_dict";
			hive_stmt.executeQuery(query);
			System.out.println("success!");

			System.out.print("\t'tf_vec' ... ");
			query = "drop table tf_vec";
			hive_stmt.executeQuery(query);
			System.out.println("success!");
		}

		hive_stmt.close();
	}


	/*
	 * Construct a warehouse over the input datasets for querying.
	 */
	private void createHiveTables() throws SQLException {

		Statement hive_stmt = hive.createStatement();

		String query;
		StringBuilder queryBuilder;

		// Construct table: metadata ==========================================
		System.out.print("[INFO]: Creating Hive table 'metadata' ... ");
		{
			queryBuilder = new StringBuilder();
			queryBuilder
			.append("create external table metadata ( name2id string ) ");
			queryBuilder.append("stored as ");
			queryBuilder.append("inputformat '" + inputFormat + "' ");
			queryBuilder.append("outputformat '" + outputFormat + "' ");
			queryBuilder.append("location '" + dirMetadata + "'");
			query = queryBuilder.toString();
			hive_stmt.executeQuery(query);
		}
		System.out.println("success!");

		// Construct table: exemplars =========================================
		System.out.print("[INFO]: Creating Hive table 'exemplars' ... ");
		{
			queryBuilder = new StringBuilder();
			queryBuilder
			.append("create external table exemplars ( exemplar int, docid int, level int ) ");
			queryBuilder.append("row format delimited ");
			queryBuilder.append("fields terminated by '\t' ");
			queryBuilder.append("lines terminated by '\n' ");
			queryBuilder.append("stored as sequencefile ");
			queryBuilder.append("location '" + dirAPOut + "'");
			query = queryBuilder.toString();
			hive_stmt.executeQuery(query);
		}
		System.out.println("success!");

		// Construct table: word_dict =========================================
		System.out.print("[INFO]: Creating Hive table 'word_dict' ... ");
		{
			queryBuilder = new StringBuilder();
			queryBuilder
			.append("create external table word_dict ( name2id string ) ");
			queryBuilder.append("stored as ");
			queryBuilder.append("inputformat '" + inputFormat + "' ");
			queryBuilder.append("outputformat '" + outputFormat + "' ");
			queryBuilder.append("location '" + dirWordDict + "'");
			query = queryBuilder.toString();
			hive_stmt.executeQuery(query);
		}
		System.out.println("success!");

		// Construct table: tf_vec ============================================
		System.out.print("[INFO]: Creating Hive table 'tf_vec' ... ");
		{
			queryBuilder = new StringBuilder();
			queryBuilder
			.append("create external table tf_vec ( docid int, wordid int, wordfreq int ) ");
			queryBuilder.append("stored as sequencefile ");
			queryBuilder.append("location '" + dir_joinWordFreq + "'");
			query = queryBuilder.toString();
			hive_stmt.executeQuery(query);
		}
		System.out.println("success!");

		hive_stmt.close();
	}


	/*
	 * Gather the necessary information to perform the ins_document
	 * query. If specified, execute the query in batch format.
	 */
	private void ins_document(boolean execute) throws SQLException {

		CallableStatement ins_document = mysql
				.prepareCall("{call ins_document(?,?,?,?,?)}");

		Statement hive_stmt = hive.createStatement();

		String query;
		ResultSet res;

		// Grab data from table: metadata
		System.out.print("[INFO]: Grabbing entries from 'metadata' ... ");
		query = "select * from metadata";
		res = hive_stmt.executeQuery(query);
		System.out.println("success!");

		// Construct query: ins_document
		System.out
		.print("[INFO]: Constructing batch SQL query for 'ins_document' ... ");
		while (res.next()) {
			String[] row = res.getString(1).split("\t");

			int docid = Integer.valueOf(row[1]);
			String docsrc = row[0];
			String docdate = "2013.06.12"; // TODO: define date column in input
			// data

			// INPUT
			ins_document.setInt(1, docid); // set document id
			ins_document.setInt(2, dataset); // set dataset
			ins_document.setString(3, docsrc); // set docsrc
			ins_document.setString(4, docdate); // set docdate

			// OUTPUT
			// ins_document.registerOutParameter(5, Types.INTEGER);
			ins_document.setInt(5, -1);

			// add to documentMetadata batch
			ins_document.addBatch();
		}
		System.out.println("success!");

		// Execute batch: ins_document
		if (execute) {
			try {
				System.out.print("[INFO]: Executing batch ins_document ... ");
				ins_document.executeBatch();
				ins_document.clearBatch();
				System.out.println("success!");
			} catch (BatchUpdateException e) {
				System.err
				.println("\n[ERROR]: Failed to complete query execution!");
				System.err.println(e);
			}
		}

		ins_document.close();
		hive_stmt.close();

	}


	/*
	 * Poll the mySQL database to access the set of UUIDs which have been
	 * freshly generated by the ins_document query. If none are returned, 
	 * the rest of this script cannot occur.
	 */
	private Map<String, String> getUUIDs() throws SQLException {

		Statement mysql_stmt = mysql.createStatement();

		// Grab a copy of the new UUID->ID mapping now on the SQL server
		// Note: This will construct the list of UUIDs. Unfortunately, the JDBC
		// API does not allow retrieval of procedure output when run in a
		// batch, so we must manually retrieve the UUID->ID listing and
		// store it in memory for later use.
		System.out.print("[INFO]: Constructing map of DocUUID -> DocID ...");
		String getUUIDs = "SELECT `uuidDocument`,`idDocument` FROM `tblDocumentLibrary` "
				+ "WHERE `docDataset`=" + dataset;
		ResultSet rows = mysql_stmt.executeQuery(getUUIDs);
		Map<String, String> uuids = new HashMap<String, String>();
		boolean atLeastOneEntry = false;
		while (rows.next()) {
			String key = rows.getString("idDocument");
			String val = rows.getString("uuidDocument");
			uuids.put(key, val);
			atLeastOneEntry = true;
		}
		System.out.println("success!");

		mysql_stmt.close();
		return atLeastOneEntry ? uuids : null;
	}


	/*
	 * Poll the mySQL database to access the current dataset ID in order 
	 * to avoid overwriting existing stored data.
	 */
	private int getDataset() throws SQLException {

		Statement mysql_stmt = mysql.createStatement();

		// Grab a copy of the current dataset
		System.out.print("[INFO]: Grabbing dataset number to use ...");
		String getUUIDs = "SELECT `docDataset` FROM `tblDocumentLibrary`";
		ResultSet rows = mysql_stmt.executeQuery(getUUIDs);

		// push the pointer up to the last result and nab it
		while (rows.next()) {
			dataset = rows.getInt(1);
		}

		System.out.println("success!");

		mysql_stmt.close();

		return dataset;
	}


	/*
	 * Gather the necessary information to perform the ins_word
	 * query. If specified, execute the query in batch format.
	 */
	private void ins_word(boolean execute) throws SQLException {

		CallableStatement ins_word = mysql
				.prepareCall("{call ins_word(?,?,?,?)}");

		Statement hive_stmt = hive.createStatement();

		String query;
		ResultSet res;

		// Grab data from table: word_dict
		System.out.print("[INFO]: Grabbing entries from 'word_dict' ... ");
		query = "select * from word_dict";
		res = hive_stmt.executeQuery(query);
		System.out.println("success!");

		// Construct query: ins_word
		System.out
		.print("[INFO]: Constructing batch SQL query for 'ins_word' ... ");
		while (res.next()) {
			String[] row = res.getString(1).split("\t");

			int wordid = Integer.valueOf(row[1]);
			String wordvalue = row[0];

			// INPUT
			ins_word.setInt(1, wordid); // word id
			ins_word.setInt(2, dataset); // dataset
			ins_word.setString(3, wordvalue); // word value

			// OUTPUT
			// ins_word.registerOutParameter(4, Types.INTEGER);
			ins_word.setInt(4, -1);

			// add to word batch
			ins_word.addBatch();
		}
		System.out.println("success!");

		// Execute Batch: ins_word
		if (execute) {
			try {
				System.out.print("[INFO]: Executing batch ins_word ... ");
				ins_word.executeBatch();
				ins_word.clearBatch();
				System.out.println("success!");
			} catch (BatchUpdateException e) {
				System.err
				.println("\n[ERROR]: Failed to complete query execution!");
				System.err.println(e);
			}
		}

		ins_word.close();
		hive_stmt.close();

	}


	/*
	 * Gather the necessary information to perform the ins_wordFreq
	 * query. If specified, execute the query in batch format.
	 */
	private void ins_wordFreq(boolean execute) throws SQLException {

		if (uuids == null) {
			System.err.println("[WARN]: No UUID map found.");
			System.err.println("[WARN]: Unable to run ins_wordFreq.");
			return;
		}

		CallableStatement ins_wordFreq = mysql
				.prepareCall("{call ins_wordFreq(?,?,?,?)}");

		try {

			Path TFVecDir = new Path(dirTFVectors);
			FileStatus[] statuses = fs.listStatus(TFVecDir);
			List<Path> paths = new ArrayList<Path>();

			// grab any non-empty files in this directory
			for (FileStatus status : statuses) {
				boolean isdir = status.isDir();
				boolean isempty = status.getLen() == 0;
				if (!isdir && !isempty) {
					paths.add(status.getPath());
				}
			}

			// for each valid file, stream its contents through a parser
			// into the hive table.
			SequenceFile.Reader in;
			Text key;
			String keyStr;
			int keyInt;
			VectorWritable val;
			String vector, valStrRaw;
			Map<Integer, Integer> wordFrequencies;

			for (Path path : paths) {

				// begin reading in from this file
				in = new SequenceFile.Reader(fs, path, conf);
				key = new Text();
				val = new VectorWritable();

				// iterate over each <key,val> pair in this file
				System.out
				.print("[INFO]: Constructing batch SQL query for 'ins_wordFreq' ... ");
				while (in.next(key, val)) {

					// grab the key (docid)
					keyStr = key.toString().substring(1);
					keyInt = Integer.parseInt(keyStr);
					keyStr = keyInt + "";
					int docuuid = Integer.valueOf(uuids.get(keyStr));

					// grab the val (vector)
					valStrRaw = val.toString();
					vector = valStrRaw.substring(1, valStrRaw.length() - 1);

					// read in the val vector based on its formatting
					// Note: this map is formatted as WORDID->WORDFREQ
					// Note: we are sorting this map based on keyvalues here
					wordFrequencies = parseTFVector(vector);
					List<Map.Entry<Integer, Integer>> entries = 
							new ArrayList<Map.Entry<Integer, Integer>>(
									wordFrequencies.entrySet());
					Collections.sort(entries, new WordFreqMapComparator());

					// Add the top N words to the batch SQL query.
					int wordID, wordFreq;
					int index = 0;
					for (Map.Entry<Integer, Integer> entry : entries) {
						if (index < Integer.valueOf(topNWords)) {
							wordID = entry.getKey();
							wordFreq = entry.getValue();

							// place docID, wordID, wordFreq into hive db
							// INPUT
							ins_wordFreq.setInt(1, docuuid);
							ins_wordFreq.setInt(2, wordID);
							ins_wordFreq.setInt(3, wordFreq);

							// OUTPUT
							// ins_wordFreq.registerOutParameter(4,
							// Types.INTEGER);
							ins_wordFreq.setInt(4, -1);

							// add to wordFreq batch
							ins_wordFreq.addBatch();

							index++;
						} else {
							break;
						}
					}
				}
				System.out.println("success!");

				// Execute Batch: ins_wordFreq
				if (execute) {
					try {
						System.out
						.print("[INFO]: Executing batch ins_wordFreq ... ");
						ins_wordFreq.executeBatch();
						ins_wordFreq.clearBatch();
						System.out.println("success!");
					} catch (BatchUpdateException e) {
						System.err
						.println("\n[ERROR]: Failed to complete query execution!");
						System.err.println(e);
					}
				}

				in.close();
			}

		} catch (IOException ioe) {
			System.err.println("[ERROR]: Unable to read data from TF-Vectors.");
			System.err.println(ioe);
			return;
		}

		ins_wordFreq.close();
	}


	/*
	 * Parse the annoyingly formatted Term Frequency vectors and store them
	 * in memory for the time being.
	 */
	private Map<Integer, Integer> parseTFVector(String vector) {
		Map<Integer, Integer> wordFrequencies = new HashMap<Integer, Integer>();

		// split vector into entries of WORD:FREQ
		String[] entries = vector.split(",");
		int wordID, wordFreq;
		double wordValdbl;
		for (String entry : entries) {
			// split entries into WORD, FREQ
			String[] keyval = entry.split(":");
			wordID = Integer.valueOf(keyval[0]);
			wordValdbl = Double.valueOf(keyval[1]);
			wordFreq = (int) wordValdbl;

			wordFrequencies.put(wordID, wordFreq);
		}

		return wordFrequencies;
	}


	/*
	 * Used to sort word frequencies by frequency, in order to find the 
	 * most frequent term in a document.
	 */
	private static class WordFreqMapComparator implements
	Comparator<Map.Entry<Integer, Integer>> {
		@Override
		public int compare(Map.Entry<Integer, Integer> a,
				Map.Entry<Integer, Integer> b) {
			return -1 * (a.getValue().compareTo(b.getValue()));
		}
	}


	/*
	 * Gather the necessary information to perform the ins_cluster
	 * query. If specified, execute the query in batch format.
	 */
	private void ins_cluster(boolean execute) throws SQLException {

		if (uuids == null) {
			System.err.println("[WARN]: No UUID map found.");
			System.err.println("[WARN]: Unable to run ins_cluster.");
			return;
		}

		CallableStatement ins_cluster = mysql
				.prepareCall("{call ins_cluster(?,?,?,?)}");

		Statement hive_stmt = hive.createStatement();

		String query;
		ResultSet res;

		// Grab data from table: exemplars
		System.out.print("[INFO]: Grabbing entries from 'exemplars' ... ");
		query = "select * from exemplars";
		res = hive_stmt.executeQuery(query);
		System.out.println("success!");

		// Call ins_cluster
		System.out
		.print("[INFO]: Constructing batch SQL query for 'ins_cluster' ... ");
		while (res.next()) {

			int aplevel = res.getInt(3) + 1; // grab level ID, account for
			// 0-index
			int clusterID = res.getInt(1); // grab cluster ID (exemplar)

			String clusterIDstr = clusterID + "";
			String clusterUUIDstr = uuids.get(clusterIDstr);
			int clusterUUID = Integer.valueOf(clusterUUIDstr);

			String docid = res.getString(2); // grab docid
			String storedUUID = uuids.get(docid); // grab uuid from map
			if (storedUUID == null) {
				System.err
				.println("\n[ERROR]: Document ID not found in UUID map. Terminating . . .");
				System.exit(1);
			}
			int docUUID = Integer.valueOf(storedUUID); // doc UUID

			if (clusterID == -1) {
				System.err
				.print("\n[ERROR]: Invalid cluster found (<0). AP Execution must have failed. Skipping . . . ");
				continue;
			}
			// INPUT
			ins_cluster.setInt(1, aplevel); // AP Level
			ins_cluster.setInt(2, clusterUUID); // cluster UUID
			ins_cluster.setInt(3, docUUID); // doc ID

			// OUTPUT
			// ins_cluster.registerOutParameter(4, Types.INTEGER);
			ins_cluster.setInt(4, -1);

			// add to word batch
			ins_cluster.addBatch();

		}
		System.out.println("success!");

		// Execute Batch: ins_cluster
		if (execute) {
			try {
				System.out.print("[INFO]: Executing batch ins_cluster ... ");
				ins_cluster.executeBatch();
				ins_cluster.clearBatch();
				System.out.println("success!");
			} catch (BatchUpdateException e) {
				System.err
				.println("\n[ERROR]: Failed to complete query execution!");
				System.err.println(e);
			}
		}

		ins_cluster.close();
		hive_stmt.close();

	}


	/*
	 * Gather the necessary information to perform the ins_clusterSize
	 * query. If specified, execute the query in batch format.
	 */
	private void ins_clusterSize(boolean execute) throws SQLException {

		if (uuids == null) {
			System.err.println("[WARN]: No UUID map found.");
			System.err.println("[WARN]: Unable to run ins_clusterSize.");
			return;
		}

		CallableStatement ins_clusterSize = mysql
				.prepareCall("{call ins_clusterSize(?,?,?,?)}");

		Statement mysql_stmt = mysql.createStatement();

		// For each UUID, grab the rows where it's found in tblClusterLibrary
		// under the column ClusterID. This represents the size of the cluster.
		String clusterCountBase = 
				"SELECT COUNT(`uuidDocument`) FROM " + 
						"`tblClusterLibrary` WHERE `idCluster`=";
		String clusterCountSuffix = " AND `APLevel`=";
		String clusterCountQuery;
		ResultSet clusterCountResults;

		String maxLevelBase = "SELECT MAX(`APLevel`) FROM " + 
				"`tblClusterLibrary` WHERE `idCluster`=";
		String maxLevelQuery;
		int maxlevel;
		ResultSet maxLevelResults;

		// Call ins_cluster
		System.out.print("[INFO]: Constructing batch SQL query " + 
				"for 'ins_clusterSize' ... ");
		Set<String> uuidKeys = uuids.keySet();
		for (String id : uuidKeys) {

			// convert docID -> docUUID
			String uuidstr = uuids.get(id);
			int uuid = Integer.valueOf(uuidstr);

			maxLevelQuery = maxLevelBase + uuid;
			maxLevelResults = mysql_stmt.executeQuery(maxLevelQuery);
			maxLevelResults.next();

			maxlevel = maxLevelResults.getInt(1);
			for (int level = 1; level <= maxlevel; level++) { // 1-indexed
				// levels
				clusterCountQuery = clusterCountBase + uuid;
				clusterCountQuery += clusterCountSuffix + level;
				clusterCountResults = mysql_stmt
						.executeQuery(clusterCountQuery);
				clusterCountResults.next();

				int size = Integer.valueOf(clusterCountResults.getInt(1));

				// INPUT
				ins_clusterSize.setInt(1, uuid); // cluster ID
				ins_clusterSize.setInt(2, level); // AP Level
				ins_clusterSize.setInt(3, size); // cluster size

				// OUTPUT
				// ins_clusterSize.registerOutParameter(4, Types.INTEGER);
				ins_clusterSize.setInt(4, -1);

				// add to clusterSize batch
				ins_clusterSize.addBatch();
			}

		}
		System.out.println("success!");

		// Execute Batch: ins_clusterSize
		if (execute) {
			try {
				System.out
				.print("[INFO]: Executing batch ins_clusterSize ... ");
				ins_clusterSize.executeBatch();
				ins_clusterSize.clearBatch();
				System.out.println("success!");
			} catch (BatchUpdateException e) {
				System.err
				.println("\n[ERROR]: Failed to complete query execution!");
				System.err.println(e);
			}
		}

		mysql_stmt.close();
		ins_clusterSize.close();

	}


	/**
	 * Redirects user input to be parsed and used as configuration values.
	 * 
	 * @param args User arguments
	 */
	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new OutputJob(), args);
		System.exit(res);
	}

}
