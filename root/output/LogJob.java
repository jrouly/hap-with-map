/**
 * Copyright 2013 AMALTHEA REU; Dillon Rose; Michel Rouly
 * 
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package root.output;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.net.URI;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.util.ToolRunner;
import org.apache.mahout.common.AbstractJob;


/**
 * <p>
 * This is the driver class for a Logging utility.
 * </p>
 * 
 * <p>
 * This job is used to extract data generated by the execution of this software
 * and store it locally, near the input dataset, with a unique identifier. This
 * preserves any data that may otherwise be lost if the database integration 
 * step is not run.
 * </p>
 * 
 * @author Dillon Rose
 * @author Michel Rouly
 * 
 * @since 2013.06.25
 * 
 */
public class LogJob extends AbstractJob {

	// -------------------------------------------------------------------
	// The following configuration variables will have default values.

	// -------------------------------------------------------------------
	// The following configuration variables must be set by the user.

	// These are the names of the initial input and final output datasets
	// to be used. Must be specified by the user.
	private static String clustersDirectory;
	private static String timeFile;
	private static String fileDictFile;
	private static String s3Directory;

	// -------------------------------------------------------------------
	// The following are temporary environment variables, not to be configured.


	/*
	 * Construct arguments list.
	 */
	private void addArguments() {

		addOption("clusters", "c", "Clusters Directory", true);
		addOption("time", "t", "Time File", true);
		addOption("s3", "s", "S3 Directory", true);
		addOption("fileDict", "f", "File Dicitonary File", true);

	}


	/*
	 * Grab arguments from the user.
	 */
	private void initArguments() {

		clustersDirectory = getOption("clusters");
		clustersDirectory = cleanDirectoryName(clustersDirectory);
		timeFile = getOption("time");
		timeFile = cleanDirectoryName(timeFile);
		s3Directory = getOption("s3");
		s3Directory = cleanDirectoryName(s3Directory);
		fileDictFile = getOption("fileDict");
		fileDictFile = cleanDirectoryName(fileDictFile);

	}


	/*
	 * Clean a directory name to remove initial '/' characters.
	 */
	private static String cleanDirectoryName(String dirName) {
		if (dirName.charAt(dirName.length() - 1) == '/') {
			dirName = dirName.substring(0, dirName.length() - 1);
		}
		return dirName;
	}


	/**
	 * This method allows the Job to act as a {@link ToolRunner} and 
	 * interface properly with the Driver.
	 * 
	 * @param args Configuration arguments
	 * @return Exit status
	 * @see ToolRunner
	 */
	@Override
	public int run(String[] args) throws Exception {

		addArguments();

		if (parseArguments(args) == null) {
			return -1;
		}

		initArguments();

		Configuration conf = getConf();

		printS3JobArgs();

		URI workingURI = new URI(conf.get("fs.default.name"));
		URI S3URI = new URI(s3Directory);

		FileSystem HDFS = FileSystem.get(workingURI, conf);
		FileSystem S3FS = FileSystem.get(S3URI, conf);

		String folderName = System.currentTimeMillis() + "";

		S3FS.mkdirs(new Path(folderName));

		Text key = new Text();
		Text value = new Text();
		int counter = 0;

		// WRITE CLUSTERS TO DISK
		FileStatus[] files = HDFS.listStatus(new Path(clustersDirectory));

		for (FileStatus f : files) {
			if (f.getPath().toString().contains("part")) {
				SequenceFile.Writer clustersWriter = new SequenceFile.Writer(
						S3FS, conf, new Path(folderName + "/clusters/"
								+ counter++), Text.class, Text.class);
				SequenceFile.Reader clustersReader = new SequenceFile.Reader(
						HDFS, f.getPath(), conf);
				key = new Text();
				value = new Text();
				while (clustersReader.next(key, value)) {
					clustersWriter.append(key, value);
				}

				clustersReader.close();
				clustersWriter.close();
			}
		}

		// WRITE FILE-DICT TO DISK
		SequenceFile.Reader fileDictReader = new SequenceFile.Reader(HDFS,
				new Path(fileDictFile), conf);

		SequenceFile.Writer fileDictWriter = new SequenceFile.Writer(S3FS,
				conf, new Path(folderName + "/file-dictionary"), Text.class,
				Text.class);

		key = new Text();
		value = new Text();
		while (fileDictReader.next(key, value)) {
			fileDictWriter.append(key, value);
		}

		fileDictReader.close();
		fileDictWriter.close();

		// WRITE TIMESTAMP TO DISK
		FSDataInputStream timeInStream = HDFS.open(new Path(timeFile));
		BufferedReader timeReader = new BufferedReader(new InputStreamReader(
				timeInStream));

		FSDataOutputStream timeOutStream = S3FS.create(new Path(folderName
				+ "/time"));
		BufferedWriter timeWriter = new BufferedWriter(new OutputStreamWriter(
				timeOutStream));

		String copyLine;
		while ((copyLine = timeReader.readLine()) != null) {
			timeWriter.write(copyLine + "\n");
		}

		timeReader.close();
		timeWriter.close();

		return 0;
	}


	/*
	 * Print the configuration values to standard output.
	 */
	private void printS3JobArgs() {
		System.out.println();
		System.out.println("=================");
		System.out.println("Running LogJob");
		System.out.println("=================");
		System.out.println("\t-s\t\t" + s3Directory);
		System.out.println("\t-c\t\t" + clustersDirectory);
		System.out.println("\t-t\t\t" + timeFile);
		System.out.println("\t-f\t\t" + fileDictFile);
		System.out.println();
	}


	/**
	 * Redirects user input to be parsed and used as configuration values.
	 * 
	 * @param args User arguments
	 */
	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new LogJob(), args);
		System.exit(res);
	}

}